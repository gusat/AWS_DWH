{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db2d4819",
   "metadata": {},
   "source": [
    "# Spin up cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0e22e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "#user => Plug here your own credentials for AWS user \n",
    "KEY = \"AKIA6G34S37L2QUROXIR\"\n",
    "SECRET = \"wI2t4f2mZhzp0DIHjQb4nkF6W3yAKBoO5EMEpwMi\"\n",
    "\n",
    "AWS_REGION = 'us-west-2'\n",
    "\n",
    "# data warehouse\n",
    "DWH_CLUSTER_TYPE = \"multi-node\"\n",
    "DWH_NUM_NODES = 2\n",
    "DWH_NODE_TYPE = \"dc2.large\"\n",
    "\n",
    "DWH_IAM_ROLE_NAME = \"dwhRole\"\n",
    "DWH_CLUSTER_IDENTIFIER = \"dwhCluster\"\n",
    "DWH_DB = \"dwh\"\n",
    "DWH_DB_USER = \"dwhuser\"\n",
    "DWH_DB_PASSWORD = \"Passw0rd\"\n",
    "DWH_PORT = 5439\n",
    "\n",
    "\n",
    "# clients for EC2, S3, IAM, Redshift\n",
    "ec2 = boto3.resource('ec2',\n",
    "                     region_name=AWS_REGION,\n",
    "                     aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                    region_name=AWS_REGION,\n",
    "                    aws_access_key_id=KEY,\n",
    "                    aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                   aws_secret_access_key=SECRET,\n",
    "                   region_name=AWS_REGION\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                        region_name=AWS_REGION,\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET\n",
    "                       )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c58e1f7a",
   "metadata": {},
   "source": [
    "Uncomment to clean-up before creating a new IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35724cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating IAM role\n",
      "IAM role already exists\n"
     ]
    }
   ],
   "source": [
    "# # Clean up residual resources (if any)\n",
    "# try:\n",
    "#     iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "#     iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "# except Exception as e:\n",
    "#     pass\n",
    "\n",
    "# Create new IAM role\n",
    "try:\n",
    "    print('Creating IAM role')\n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description=\"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {\n",
    "                'Statement': [\n",
    "                    {\n",
    "                        'Action': 'sts:AssumeRole',\n",
    "                        'Effect': 'Allow',\n",
    "                        'Principal': {'Service': 'redshift.amazonaws.com'}\n",
    "                    }\n",
    "                ],\n",
    "                'Version': '2012-10-17'\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    print('IAM role created successfully')\n",
    "\n",
    "except iam.exceptions.EntityAlreadyExistsException:\n",
    "    print('IAM role already exists')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error creating IAM role: {e}')\n",
    "\n",
    "\n",
    "    # Attach policy to the IAM role\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
    "    )\n",
    "    print('Policy attached successfully')\n",
    "\n",
    "    # Get IAM role ARN\n",
    "    role_arn = dwhRole['Role']['Arn']\n",
    "    print('IAM role ARN:', role_arn)\n",
    "\n",
    "except iam.exceptions.EntityAlreadyExistsException:\n",
    "    print('IAM role already exists')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error creating IAM role: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44969e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy attached successfully.\n",
      "IAM role ARN: arn:aws:iam::976829472727:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "# https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum-create-role.html\n",
    "\n",
    "# Create an IAM client\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "try:\n",
    "    # Attach policy allowing access to all Amazon S3 buckets\n",
    "    policy_arn = \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "    response = iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=policy_arn)\n",
    "    if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        print(\"Policy attached successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to attach policy.\")\n",
    "\n",
    "    # Get the IAM role ARN\n",
    "    response = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "    role_arn = response['Role']['Arn']\n",
    "    print(\"IAM role ARN: \" + role_arn)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8dc5385",
   "metadata": {},
   "source": [
    "# (delete before) Creating a new RS Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c06c682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing cluster: dwhCluster\n",
      "Cluster status: deleting\n",
      "Cluster deleted: dwhCluster\n",
      "Cluster deletion time: 63.60 seconds\n",
      "Create Redshift cluster\n",
      "\n",
      "{'Cluster': {'ClusterIdentifier': 'dwhcluster', 'NodeType': 'dc2.large', 'ClusterStatus': 'creating', 'ClusterAvailabilityStatus': 'Modifying', 'MasterUsername': 'dwhuser', 'DBName': 'dwh', 'AutomatedSnapshotRetentionPeriod': 1, 'ManualSnapshotRetentionPeriod': -1, 'ClusterSecurityGroups': [], 'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-047724a125a9baa79', 'Status': 'active'}], 'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0', 'ParameterApplyStatus': 'in-sync'}], 'ClusterSubnetGroupName': 'default', 'VpcId': 'vpc-02968d43e458734d2', 'PreferredMaintenanceWindow': 'wed:08:30-wed:09:00', 'PendingModifiedValues': {'MasterUserPassword': '****'}, 'ClusterVersion': '1.0', 'AllowVersionUpgrade': True, 'NumberOfNodes': 2, 'PubliclyAccessible': True, 'Encrypted': False, 'Tags': [], 'EnhancedVpcRouting': False, 'IamRoles': [{'IamRoleArn': 'arn:aws:iam::976829472727:role/dwhRole', 'ApplyStatus': 'adding'}], 'MaintenanceTrackName': 'current', 'DeferredMaintenanceWindows': [], 'NextMaintenanceWindowStartTime': datetime.datetime(2023, 6, 28, 8, 30, tzinfo=tzutc()), 'AquaConfiguration': {'AquaStatus': 'disabled', 'AquaConfigurationStatus': 'auto'}}, 'ResponseMetadata': {'RequestId': 'cd1b048d-ffed-45ad-88c0-8dd5cb5a6591', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'cd1b048d-ffed-45ad-88c0-8dd5cb5a6591', 'content-type': 'text/xml', 'content-length': '2445', 'date': 'Sun, 25 Jun 2023 08:15:33 GMT'}, 'RetryAttempts': 0}}\n",
      "Cluster status: creating\n",
      "Cluster status: creating\n",
      "Cluster status: available\n",
      "Cluster creation completed successfully.\n",
      "Cluster creation time: 124.69 seconds\n"
     ]
    }
   ],
   "source": [
    "# Delete AWS Redshift cluster if it may already exists \n",
    "start_time_delete = time.time()\n",
    "try:\n",
    "    redshift.delete_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True)\n",
    "    print(f\"Deleted existing cluster: {DWH_CLUSTER_IDENTIFIER}\")\n",
    "except redshift.exceptions.ClusterNotFoundFault:\n",
    "    print(f\"No existing cluster found: {DWH_CLUSTER_IDENTIFIER}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting cluster: {e}\")\n",
    "\n",
    "# Wait for cluster deletion to complete\n",
    "while True:\n",
    "    try:\n",
    "        response = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\n",
    "        cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "        print(f\"Cluster status: {cluster_status}\")\n",
    "    except redshift.exceptions.ClusterNotFoundFault:\n",
    "        print(f\"Cluster deleted: {DWH_CLUSTER_IDENTIFIER}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking cluster status: {e}\")\n",
    "    \n",
    "    time.sleep(60)  # Wait for 1 minute before checking again\n",
    "\n",
    "# Calculate cluster deletion time\n",
    "end_time_delete = time.time()\n",
    "deletion_time = end_time_delete - start_time_delete\n",
    "print(f\"Cluster deletion time: {deletion_time:.2f} seconds\")\n",
    "\n",
    "# Create a new cluster\n",
    "start_time_create = time.time()\n",
    "try:\n",
    "    print(\"Create Redshift cluster\\n\")\n",
    "    resp = redshift.create_cluster(\n",
    "        # HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        # Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "\n",
    "        # Roles (for S3 access)\n",
    "        IamRoles=[role_arn]\n",
    "    )\n",
    "    print(resp)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating cluster: {e}\")\n",
    "\n",
    "# Wait for cluster creation to complete\n",
    "while True:\n",
    "    try:\n",
    "        response = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\n",
    "        cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "        print(f\"Cluster status: {cluster_status}\")\n",
    "        if cluster_status == 'available':\n",
    "            print(\"Cluster creation completed successfully.\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking cluster status: {e}\")\n",
    "    \n",
    "    time.sleep(60)  # Wait for 1 minute before checking again\n",
    "\n",
    "# Calculate cluster creation time\n",
    "end_time_create = time.time()\n",
    "creation_time = end_time_create - start_time_create\n",
    "print(f\"Cluster creation time: {creation_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9357d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster is available\n",
      "Cluster properties retrieved successfully\n",
      "{'ClusterIdentifier': 'dwhcluster', 'NodeType': 'dc2.large', 'ClusterStatus': 'available', 'ClusterAvailabilityStatus': 'Available', 'MasterUsername': 'dwhuser', 'DBName': 'dwh', 'Endpoint': {'Address': 'dwhcluster.cdey2alnjog5.us-west-2.redshift.amazonaws.com', 'Port': 5439}, 'ClusterCreateTime': datetime.datetime(2023, 6, 25, 8, 17, 12, 811000, tzinfo=tzutc()), 'AutomatedSnapshotRetentionPeriod': 1, 'ManualSnapshotRetentionPeriod': -1, 'ClusterSecurityGroups': [], 'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-047724a125a9baa79', 'Status': 'active'}], 'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0', 'ParameterApplyStatus': 'in-sync'}], 'ClusterSubnetGroupName': 'default', 'VpcId': 'vpc-02968d43e458734d2', 'AvailabilityZone': 'us-west-2d', 'PreferredMaintenanceWindow': 'wed:08:30-wed:09:00', 'PendingModifiedValues': {}, 'ClusterVersion': '1.0', 'AllowVersionUpgrade': True, 'NumberOfNodes': 2, 'PubliclyAccessible': True, 'Encrypted': False, 'ClusterPublicKey': 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCUkqpYU5ehHhyLSr7dX412RLH1P2J44UGhnNK1byr2zaMgU8QeQS0hEe+Nc+U5NhbrHrZMvCEguO7ej1XpEdymJN5IREZ7zfIBe2Ly5VsoEs3PqAOsYUbnE5Zjcxgvr7NMq5sifJbm++XKBX5mGCJE43/OlRujttUGT3dEZhokqLSkIsreB49QpS624FFoLU2GBpZE6BP6ZYiqGNd9Aczbei8pAUXchseWXJox8XpxiSJzFodRYmXim4w2SrndQN4NxNx/+Tufz5MCS02qD99z+j87hHdkYFNOku3EqDifCK7nH17JojqKyrSshD1Vw2Zn+Sf5Wna/BPc0Q5VIXyrH Amazon-Redshift\\n', 'ClusterNodes': [{'NodeRole': 'LEADER', 'PrivateIPAddress': '172.31.51.55', 'PublicIPAddress': '44.229.133.216'}, {'NodeRole': 'COMPUTE-0', 'PrivateIPAddress': '172.31.53.206', 'PublicIPAddress': '44.238.55.108'}, {'NodeRole': 'COMPUTE-1', 'PrivateIPAddress': '172.31.59.94', 'PublicIPAddress': '52.27.138.69'}], 'ClusterRevisionNumber': '51973', 'Tags': [], 'EnhancedVpcRouting': False, 'IamRoles': [{'IamRoleArn': 'arn:aws:iam::976829472727:role/dwhRole', 'ApplyStatus': 'in-sync'}], 'MaintenanceTrackName': 'current', 'ElasticResizeNumberOfNodeOptions': '[4]', 'DeferredMaintenanceWindows': [], 'NextMaintenanceWindowStartTime': datetime.datetime(2023, 6, 28, 8, 30, tzinfo=tzutc()), 'AvailabilityZoneRelocationStatus': 'disabled', 'ClusterNamespaceArn': 'arn:aws:redshift:us-west-2:976829472727:namespace:e37f7506-3bf3-4f59-b2c4-5dfb2ec8758e', 'AquaConfiguration': {'AquaStatus': 'disabled', 'AquaConfigurationStatus': 'auto'}}\n",
      "\n",
      "You can check the cluster in the AWS console: https://console.aws.amazon.com/redshiftv2/home?region=us-west-2#cluster-details/dwhCluster\n"
     ]
    }
   ],
   "source": [
    "# Create an Amazon Redshift client\n",
    "redshift = boto3.client('redshift', region_name=AWS_REGION)\n",
    "\n",
    "try:\n",
    "    # Wait for the cluster to become available\n",
    "    while True:\n",
    "        cluster_status = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]['ClusterStatus']\n",
    "        if cluster_status == 'available':\n",
    "            break\n",
    "        time.sleep(5)\n",
    "\n",
    "    print('Cluster is available')\n",
    "\n",
    "    # Retrieve cluster properties\n",
    "    cluster_props = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "    print('Cluster properties retrieved successfully')\n",
    "    print(cluster_props)\n",
    "\n",
    "    # Provide the AWS console link to the cluster\n",
    "    cluster_url = f\"https://console.aws.amazon.com/redshiftv2/home?region={AWS_REGION}#cluster-details/{DWH_CLUSTER_IDENTIFIER}\"\n",
    "    print(f\"\\nYou can check the cluster in the AWS console: {cluster_url}\")\n",
    "\n",
    "except redshift.exceptions.ClusterNotFoundFault:\n",
    "    print(f'Cluster {DWH_CLUSTER_IDENTIFIER} not found')\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EC2 client\n",
    "ec2 = boto3.resource('ec2', region_name=AWS_REGION)\n",
    "\n",
    "try:\n",
    "    # Retrieve cluster properties\n",
    "    cluster_props = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "    print('Cluster properties retrieved successfully')\n",
    "\n",
    "    # Only proceed if the cluster is available\n",
    "    if cluster_props['ClusterStatus'] == 'available':\n",
    "        # Retrieve endpoint and IAM role ARN\n",
    "        DWH_ENDPOINT = cluster_props['Endpoint']['Address']\n",
    "        DWH_ROLE_ARN = cluster_props['IamRoles'][0]['IamRoleArn']\n",
    "        print(\"DWH_ENDPOINT = \", DWH_ENDPOINT)\n",
    "        print(\"DWH_ROLE_ARN = \", DWH_ROLE_ARN)\n",
    "        print()\n",
    "\n",
    "        print('Opening TCP port to endpoint')\n",
    "        vpc = ec2.Vpc(id=cluster_props['VpcId'])\n",
    "        default_sg = list(vpc.security_groups.all())[0]\n",
    "        print('Default security group: %s' % default_sg)\n",
    "\n",
    "        # Check if the rule already exists\n",
    "        existing_rules = default_sg.ip_permissions\n",
    "        port_range = f'{DWH_PORT}-{DWH_PORT}'\n",
    "        existing_rule = next((rule for rule in existing_rules if\n",
    "                              rule['IpProtocol'] == 'tcp' and\n",
    "                              rule['FromPort'] == int(DWH_PORT) and\n",
    "                              rule['ToPort'] == int(DWH_PORT)), None)\n",
    "\n",
    "        if existing_rule is None:\n",
    "            # Authorize ingress for TCP port\n",
    "            try:\n",
    "                default_sg.authorize_ingress(\n",
    "                    GroupName=default_sg.group_name,\n",
    "                    CidrIp='0.0.0.0/0',\n",
    "                    IpProtocol='TCP',\n",
    "                    FromPort=int(DWH_PORT),\n",
    "                    ToPort=int(DWH_PORT)\n",
    "                )\n",
    "                print('TCP port opened successfully')\n",
    "            except Exception as e:\n",
    "                print(f'Error opening TCP port: {e}')\n",
    "        else:\n",
    "            print('TCP port is already open')\n",
    "    else:\n",
    "        print('Cluster is not available yet')\n",
    "\n",
    "except redshift.exceptions.ClusterNotFoundFault:\n",
    "    print(f'Cluster {DWH_CLUSTER_IDENTIFIER} not found')\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270a136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLUSTER]\n",
      "HOST='dwhcluster.cdey2alnjog5.us-west-2.redshift.amazonaws.com'\n",
      "DB_NAME='dwh'\n",
      "DB_USER='dwhuser'\n",
      "DB_PASSWORD='Passw0rd'\n",
      "DB_PORT=5439\n",
      "\n",
      "[IAM_ROLE]\n",
      "ARN='arn:aws:iam::976829472727:role/dwhRole'\n",
      "\n",
      "[S3]\n",
      "LOG_DATA='s3://udacity-dend/log-data'\n",
      "LOG_JSONPATH='s3://udacity-dend/log_json_path.json'\n",
      "SONG_DATA='s3://udacity-dend/song-data/A/A/A'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This goes into dwh.cfg\n",
    "# This remains stable even when stopping and restarting resources\n",
    "# so likely only needs to be copied once.\n",
    "\n",
    "\n",
    "# Generate dwh.cfg content\n",
    "cfg_content = f\"\"\"[CLUSTER]\n",
    "HOST='{DWH_ENDPOINT}'\n",
    "DB_NAME='{DWH_DB}'\n",
    "DB_USER='{DWH_DB_USER}'\n",
    "DB_PASSWORD='{DWH_DB_PASSWORD}'\n",
    "DB_PORT={DWH_PORT}\n",
    "\n",
    "[IAM_ROLE]\n",
    "ARN='{role_arn}'\n",
    "\n",
    "[S3]\n",
    "LOG_DATA='s3://udacity-dend/log-data'\n",
    "LOG_JSONPATH='s3://udacity-dend/log_json_path.json'\n",
    "SONG_DATA='s3://udacity-dend/song-data/A/A/A'\n",
    "\"\"\"\n",
    "\n",
    "print(cfg_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4179ecf",
   "metadata": {},
   "source": [
    "# Pause (more effective) or Delete resources to stop charging $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725bf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pause AWS Redshift cluster after taking a mandatory snapshot, and waiting for its completion\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift = boto3.client('redshift', region_name=AWS_REGION)\n",
    "\n",
    "# Pause AWS Redshift cluster after deleting any existing prior (manual!) snapshots\n",
    "start_time_pause = time.time()\n",
    "\n",
    "try:\n",
    "    response = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\n",
    "    cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "    print(f\"Cluster status: {cluster_status}\")\n",
    "\n",
    "    if cluster_status == 'available':\n",
    "        snapshot_identifier = f\"{DWH_CLUSTER_IDENTIFIER}-snapshot\"\n",
    "\n",
    "        # Retrieve existing snapshots for the cluster\n",
    "        existing_snapshots = redshift.describe_cluster_snapshots(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Snapshots']\n",
    "\n",
    "        # Delete manual snapshots (exclude automatic snapshots)\n",
    "        for snapshot in existing_snapshots:\n",
    "            if snapshot['SnapshotType'] == 'manual':\n",
    "                redshift.delete_cluster_snapshot(SnapshotIdentifier=snapshot['SnapshotIdentifier'])\n",
    "                print(f\"Deleted existing snapshot: {snapshot['SnapshotIdentifier']}\")\n",
    "\n",
    "        # Create a new manual snapshot\n",
    "        redshift.create_cluster_snapshot(\n",
    "            SnapshotIdentifier=snapshot_identifier,\n",
    "            ClusterIdentifier=DWH_CLUSTER_IDENTIFIER\n",
    "        )\n",
    "        print(f\"Created manual snapshot: {snapshot_identifier}\")\n",
    "\n",
    "        # Wait for manual snapshot creation to complete\n",
    "        while True:\n",
    "            response = redshift.describe_cluster_snapshots(SnapshotIdentifier=snapshot_identifier)\n",
    "            snapshot_status = response['Snapshots'][0]['Status']\n",
    "            print(f\"Snapshot status: {snapshot_status}\")\n",
    "            if snapshot_status == 'available':\n",
    "                break\n",
    "            time.sleep(60)\n",
    "\n",
    "        # Pause the cluster\n",
    "        redshift.pause_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\n",
    "        print(f\"Paused existing cluster: {DWH_CLUSTER_IDENTIFIER}\")\n",
    "    else:\n",
    "        print(f\"Cluster is not available: {DWH_CLUSTER_IDENTIFIER}\")\n",
    "\n",
    "except redshift.exceptions.ClusterNotFoundFault:\n",
    "    print(f\"No existing cluster found: {DWH_CLUSTER_IDENTIFIER}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error pausing cluster: {e}\")\n",
    "\n",
    "# Wait for cluster pause to complete\n",
    "while True:\n",
    "    try:\n",
    "        response = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\n",
    "        cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "        print(f\"Cluster status: {cluster_status}\")\n",
    "\n",
    "        if cluster_status == 'paused':\n",
    "            print(f\"Cluster paused: {DWH_CLUSTER_IDENTIFIER}\")\n",
    "            break\n",
    "\n",
    "    except redshift.exceptions.ClusterNotFoundFault:\n",
    "        print(f\"Cluster paused: {DWH_CLUSTER_IDENTIFIER}\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking cluster status: {e}\")\n",
    "\n",
    "    time.sleep(60)  # Wait for 1 minute before checking again\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dd49b17",
   "metadata": {},
   "source": [
    "Restart / resume the PAUSED cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0daf996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster dwhCluster is being restarted\n",
      "Cluster status: resuming\n",
      "Cluster status: resuming\n",
      "Cluster status: resuming\n",
      "Cluster status: resuming\n",
      "Cluster status: resuming\n",
      "Cluster status: available\n",
      "Cluster dwhCluster is available again\n"
     ]
    }
   ],
   "source": [
    "# to resume the PAUSED cluster\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift = boto3.client('redshift', region_name=AWS_REGION)\n",
    "\n",
    "try:\n",
    "    # Resume the Redshift cluster\n",
    "    response = redshift.resume_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\n",
    "    print(f\"Cluster {DWH_CLUSTER_IDENTIFIER} is being restarted\")\n",
    "except redshift.exceptions.ClusterNotFoundFault:\n",
    "    print(f\"Cluster {DWH_CLUSTER_IDENTIFIER} not found\")\n",
    "except Exception as e:\n",
    "    print(f\"Error restarting cluster: {e}\")\n",
    "\n",
    "# Wait for cluster restart to complete\n",
    "while True:\n",
    "    try:\n",
    "        response = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\n",
    "        cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "        print(f\"Cluster status: {cluster_status}\")\n",
    "\n",
    "        if cluster_status == 'available':\n",
    "            print(f\"Cluster {DWH_CLUSTER_IDENTIFIER} is available again\")\n",
    "            break\n",
    "\n",
    "    except redshift.exceptions.ClusterNotFoundFault:\n",
    "        print(f\"Cluster {DWH_CLUSTER_IDENTIFIER} not found\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking cluster status: {e}\")\n",
    "\n",
    "    time.sleep(60)  # Wait for 1 minute before checking again\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc155116",
   "metadata": {},
   "source": [
    "# or, full Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf0dcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing cluster: dwhCluster\n",
      "Cluster status: deleting\n",
      "Cluster status: deleting\n",
      "Cluster deleted: dwhCluster\n",
      "Cluster deletion time: 124.56 seconds\n"
     ]
    }
   ],
   "source": [
    "# Delete AWS Redshift cluster if it already exists and not needed anymore\n",
    "start_time_delete = time.time()\n",
    "try:\n",
    "    redshift.delete_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True)\n",
    "    print(f\"Deleted existing cluster: {DWH_CLUSTER_IDENTIFIER}\")\n",
    "except redshift.exceptions.ClusterNotFoundFault:\n",
    "    print(f\"No existing cluster found: {DWH_CLUSTER_IDENTIFIER}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting cluster: {e}\")\n",
    "\n",
    "# Wait for cluster deletion to complete\n",
    "while True:\n",
    "    try:\n",
    "        response = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\n",
    "        cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "        print(f\"Cluster status: {cluster_status}\")\n",
    "    except redshift.exceptions.ClusterNotFoundFault:\n",
    "        print(f\"Cluster deleted: {DWH_CLUSTER_IDENTIFIER}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking cluster status: {e}\")\n",
    "    \n",
    "    time.sleep(60)  # Wait for 1 minute before checking again\n",
    "\n",
    "# Calculate cluster deletion time\n",
    "end_time_delete = time.time()\n",
    "deletion_time = end_time_delete - start_time_delete\n",
    "print(f\"Cluster deletion time: {deletion_time:.2f} seconds\")\n",
    "\n",
    "# # Delete IAM policy and role\n",
    "try:\n",
    "    iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "    iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce8d7425",
   "metadata": {},
   "source": [
    "# Clean up IAM Roles and Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33df26cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: An error occurred (NoSuchEntity) when calling the DetachRolePolicy operation: The role with name dwhRole cannot be found.\n"
     ]
    }
   ],
   "source": [
    "# # Delete IAM policy and role\n",
    "try:\n",
    "    iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "    iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e07c4598",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa2917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(\n",
    "                         DWH_ENDPOINT, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc028e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/redshift/latest/dg/r_STL_LOAD_ERRORS.html\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM stl_load_errors;\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(query)\n",
    "conn.commit()\n",
    "\n",
    "row = cur.fetchone()\n",
    "while row:\n",
    "    print(row)\n",
    "    print()\n",
    "    row = cur.fetchone()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
